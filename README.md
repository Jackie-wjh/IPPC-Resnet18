# Sportsâ€‘100 Image Classification (ResNetâ€‘18)

A reproducible PyTorch pipeline for **100â€‘class sports image classification** built on **ResNetâ€‘18**.  
The project supports configurationâ€‘driven experiments, warmâ€‘up + cosine LR scheduling, **AMP** mixed precision,
and compact error analysis via a **worstâ€‘K + other** confusion matrix.

> **Common defaults across all four experiments**: AMP enabled, **8** DataLoader workers, **batch size = 64**, and **weight decay = 0.05**.

---

## ğŸ“‚ Project structure

```
IPPC_Resnet18/
â”œâ”€ configs/                 # YAML configs for experiments
â”‚  â”œâ”€ baseline.yaml
â”‚  â”œâ”€ pretrained.yaml
â”‚  â”œâ”€ robust.yaml
â”‚  â”œâ”€ scratch_100.yaml
â”‚  â””â”€ compare.yaml
â”œâ”€ data/                    # ImageFolder-style dataset
â”‚  â”œâ”€ train/<class>/*.jpg
â”‚  â”œâ”€ valid/<class>/*.jpg
â”‚  â””â”€ test/<class>/*.jpg
â”œâ”€ figures/                 # Plots & per-class reports (generated)
â”œâ”€ runs/                    # Checkpoints & logs for each experiment
â”‚  â”œâ”€ baseline/     (best.pt, log.csv)
â”‚  â”œâ”€ pretrained/   (best.pt, log.csv)
â”‚  â”œâ”€ robust/       (best.pt, log.csv)
â”‚  â””â”€ scratch_100/  (best.pt, log.csv)
â”œâ”€ src/
â”‚  â”œâ”€ train.py              # training loop (Linear warm-up â†’ Cosine annealing)
â”‚  â”œâ”€ eval_test.py          # test-time eval + worstK+other confusion plot
â”‚  â”œâ”€ dataset.py            # ImageFolder loaders & transforms
â”‚  â”œâ”€ transforms.py         # train/val transforms (RandAugment, etc.)
â”‚  â”œâ”€ model.py              # ResNetâ€‘18 head (Identity â†’ Dropout? â†’ Linear)
â”‚  â”œâ”€ metrics.py            # TorchMetrics pack (top1/top5/F1)
â”‚  â”œâ”€ utils.py              # seeding, AverageMeter, helpers
â”‚  â”œâ”€ plot_loss.py          # (optional) loss curve plotting from log.csv
â”‚  â””â”€ compare_val_acc.py    # (optional) compare val acc across runs
â””â”€ requirements.txt
```

---

## âœ¨ Features

- **ResNetâ€‘18** backbone with optional **ImageNet pretraining**
- Strong augmentation (**RandAugment**), optional **label smoothing** and **dropout**
- **AdamW** optimizer; **Linear warmâ€‘up â†’ Cosine annealing** LR, with **linear LR scaling** by batch size
- **AMP** mixed precision for speed & memory (`torch.amp.autocast('cuda')`, `amp.GradScaler('cuda')`)
- Metrics at validation/test: **Acc@1**, **Macroâ€‘F1**, **Macroâ€‘Recall**
- Diagnostic plots: training curves, worstâ€‘K confusion matrices

---

## ğŸ“¦ Setup

```bash
# (optional) new env
# conda create -n r18 python=3.10 -y && conda activate r18

pip install --upgrade pip
pip install -r requirements.txt
```

`requirements.txt` (excerpt):
```
torch>=2.1.0
torchvision>=0.16.0
torchmetrics>=1.3.0
tqdm>=4.64
pyyaml>=6.0
numpy>=1.23
pandas>=1.5
matplotlib>=3.7
scikit-learn>=1.2
```

---

## ğŸš€ Train

**Pretrained fineâ€‘tuning**
```bash
python -m src.train --config configs/pretrained.yaml
```

**From scratch (baseline)**
```bash
python -m src.train --config configs/baseline.yaml
```

**Robust (pretraining + strong aug + smoothing + dropout)**
```bash
python -m src.train --config configs/robust.yaml
```

**Longer fromâ€‘scratch schedule**
```bash
python -m src.train --config configs/scratch_100.yaml
```

Each run writes to `runs/<exp>/`:
- `best.pt` â€” checkpoint dict with `model.state_dict()`, `classes`, `cfg`
- `log.csv` â€” columns: `epoch,train_loss,val_loss,val_top1,val_top5,val_f1,lr`

---

## ğŸ§ª Evaluate & plot (test set)

Generate **Acc@1(precision) / Macroâ€‘F1 / Macroâ€‘Recall** and a **worstâ€‘K + other** confusion matrix:

```bash
python -m src.eval_test \
  --cfg  configs/pretrained.yaml \
  --ckpt runs/pretrained/best.pt \
  --out_dir figures \
  --k_worst 20 \
  --suffix pretrained_basic

# Example outputs:
#   figures/per_class_report_pretrained_basic.csv
#   figures/confusion_worst20_pretrained_basic.png
```

Optional helpers (if you choose to use them):

```bash
# Plot loss curves from a log.csv
python -m src.plot_loss --csv runs/baseline/log.csv --out figures/loss_curves_baseline.png --label baseline

# Compare validation accuracy across runs (see configs/compare.yaml if used)
python -m src.compare_val_acc --cfg configs/compare.yaml
```

---

## ğŸ”§ Configs (YAML)

All configs share the same schema:

```yaml
seed: 312
data_root: data
out_dir: runs/<name>
batch_size: 64
num_workers: 8
pretrained: true|false
strong_aug: true|false
label_smoothing: 0.0|0.1
dropout: 0.0|0.2
lr: 3.0e-4 or 3.0e-3  # base LR; peak scales by (batch_size/64)
weight_decay: 0.05
epochs: 50|100
warmup: 10% epochs
amp: true
```

**Experiment matrix used in the report**

| Experiment (`out_dir`) | pretrained | strong_aug | label_smoothing | dropout | lr | weight_decay | epochs | warmup |
|---|:---:|:---:|:---:|:---:|---:|---:|---:|---:|
| `runs/baseline`     | âœ— | âœ” | 0.1 | 0.0 | 3eâ€‘3  | 0.05 | 50  | 5  |
| `runs/pretrained`   | âœ” | âœ— | 0.0 | 0.0 | 3eâ€‘4  | 0.05 | 50  | 5  |
| `runs/robust`       | âœ” | âœ” | 0.1 | 0.2 | 3eâ€‘4  | 0.05 | 50  | 5  |
| `runs/scratch_100`  | âœ— | âœ” | 0.1 | 0.0 | 3eâ€‘3  | 0.05 | 100 | 10 |

> LR linear scaling rule: `base_lr = cfg['lr'] * (batch_size / 64)`.

---

## ğŸ§° Tips & troubleshooting

- **AMP deprecations**: prefer `from torch import amp; amp.autocast('cuda')` and `amp.GradScaler('cuda')`.
- **Throughput**: if GPU is underâ€‘utilized, increase `num_workers` (up to CPU cores), enable `pin_memory=True` and `persistent_workers=True` in loaders.
- **Permission denied**: run with `python src/train.py ...` (or add a shebang and `chmod +x`).
- **Reproducibility**: fixed seeds for Python/NumPy/PyTorch/CUDA; set CuDNN deterministic for final report runs.

---

## ğŸ™ Acknowledgments
Built with PyTorch and torchvision. ResNetâ€‘18 follows the torchvision reference; ImageNet statistics are used for normalization.
